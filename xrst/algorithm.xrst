SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later
SPDX-FileCopyrightText: Bradley M. Bell <bradbell@seanet.com>
SPDX-FileContributor: 2023 Bradley M. Bell
-----------------------------------------------------------------------------
{xrst_begin_parent algorithm}

Algorithms Used to Compare Packages
###################################

{xrst_end algorithm}
------------------------------------------------------------------------------
{xrst_begin det_by_minor}
{xrst_spell
   th
}

Determinant Using Expansion by Minors
#####################################

ell
***
We use :math:`\ell` to denote the row and column dimension
of the square matrix under consideration.

Algorithm
*********
This algorithm computes :math:`|A|` the determinant of a square matrix
:math:`A \in \B{R}^{\ell \times \ell}` .
In the special case where *\ell* is one, the determinant
is just the element of the matrix.
Otherwise, the determinant is computed using the formula

.. math::

   | A | = + | A(0,0) | - | A(0,1) |  + | A(0,2) | \cdots \pm  | A(0,\ell-1) |

where the last term is + (-) if *\ell* is odd (even) and
:math:`A(i,j) \in \B{R}^{(\ell-1) \times (\ell-1)}` is the
:math:`(i,j)` minor of *A*\ ; i.e., the square sub-matrix formed by deleting
the *i*\ -th row and *j*\ -th column of *A* .

option
******
The only option used by this algorithm is ``n_arg`` ; see below:

n_arg
=====
This is the number of arguments to the algorithm
which is the number of elements in the matrix; i.e., math:`\ell^2` .
There is an assert checking that *n_arg* > 0.

n_other
=======
This option is only checked to make sure it is zero.
There is an assert checking that *n_other* == 0.


Implementation
==============
{xrst_toc_table
   cpp/include/cmpad/algo/det_by_minor.hpp
   python/cmpad/det_by_minor.py
}

Derivative
**********
The partial derivative of :math:`|A|` with respect to
the :math:`(i,j)`\ -th element :math:`A_{i,j}` is

.. math::

   \frac{ \partial |A| }{ \partial A_{i,j} } = (-1)^{i+j} | A(i,j) |

{xrst_end det_by_minor}
------------------------------------------------------------------------------
{xrst_begin an_ode}
{xrst_spell
   kutta
   llll
   runge
   truncation
}

An ODE Solution
###############

ODE
***

.. math::

   \begin{array}{llll}
   y_i '(t) & = & x_0                          & \mbox{for} \; i = 0 \\
   y_i '(t) & = & \sum_{j=1}^i x_j y_{i-1} (t) & \mbox{for} \; i > 0  \\
   \end{array}

Parameter Vector
================
We refer to :math:`x` as the parameter vector in the ode.

Initial Value
*************

.. math::

   y_i (0) = 0  \; \mbox{for all} \; i

Solution
********
This initial value problem has the following analytic solution
(which can be used to check function values and derivatives):

.. math::

   \begin{array}{llll}
   y_0 (t) & = & x_0 t \\
   y_1 (t) & = & x_0 x_1 t^2 / 2 ! \\
   y_2 (t) & = & x_0 x_1 x_2 t^3 / 3 !
   \end{array}

.. math ::

   y_i (t) =
      \frac{t^{i+1}}{(i+1)!} \prod_{j=0}^i x_j \; \; \mbox{for all} \; i

Algorithm
*********
The :ref:`rk4_step-name` method is used to approximate
the solution for :math:`y(t)` at :math:`t = 2` .
Note that this approximation has no truncation error for :math:`i < 4` .

option
******
This algorithm uses the ``n_arg`` and ``n_other`` options; see below:

n_arg
=====
This is the size of the vectors *x* and *y* above .
There is an assert checking that *n_arg* > 0.

n_other
=======
This is the number of :ref:`Runge-Kutta steps<rk4_step-name>`
used to approximate the solution of the ODE :math:`y(2)` .
There is an assert checking that *n_other* > 0.

Implementation
==============
{xrst_toc_table
   xrst/rk4_step.xrst
   cpp/include/cmpad/algo/an_ode.hpp
   python/cmpad/an_ode.py
}

Derivative
**********
The partial of :math:`y_i (t)` with respect to :math:`x_j` is

.. math::

   \frac{ \partial y_i (t) }{  \partial x_j } = \begin{cases}
      y_i (t) / x_j   & \text{if} \; j \leq i  \\
      0               & \text{otherwise}
   \end{cases}


{xrst_end an_ode}
------------------------------------------------------------------------------
{xrst_begin llsq_obj}
{xrst_spell
   vectorized
}

Least Squares Linear Regression Objective
#########################################

Function
********

.. math::

   y(x) = \frac{1}{2} \sum_i^n \left(
      s_i - x_0 - x_1 t_i - x_2 t_i^2 - \cdots
   \right)^2

where *s* and *t* in :math:`{\bf R}^n` are given by:

.. math::

   t_j & = -1 + i * 2 / (n - 1)
   \\
   s_j & = \mathrm{sign} ( t_j )
   =
   \cases{
      -1 & if $t_j < 0$ \\
       0 & if $t_j = 0$ \\
      +1 & if $t_j > 0$
   }


Vector Operations
*****************
The calculation of this objective can be vectorized
with respect to the index *j* above.
The intent is to enable AD packages that have vector operators
to use them to speed up the calculation of derivatives.
For example, :ref:`py_llsq_obj-name` can be used by any of
the AD packages while :ref:`pytorch_llsq_obj-name` can
only be used by the pytorch package.
Using AD package specific operations requires re-writing
source code and is not be feasible in some cases.

option
******
This algorithm uses the ``n_arg`` and ``n_other`` options; see below:

n_arg
=====
This is the size of the vector *x* above .
There is an assert checking that *n_arg* > 0.

n_other
=======
This is the size of the vector *t* above .
There is an assert checking that *n_other* > 0.

Implementation
==============
{xrst_toc_table
   cpp/include/cmpad/algo/llsq_obj.hpp
   python/cmpad/llsq_obj.py
   python/cmpad/pytorch/llsq_obj.py
}

Derivative
**********
The partial derivative of :math:`y(x)` with respect to :math:`x_j` is

.. math::

   \frac{ \partial y(x)} { \partial x_j }
   =
   - \sum_i^n \left(
      s_i - x_0 - x_1 t_i - x_2 t_i^2 - \cdots
   \right) t_i^j


{xrst_end llsq_obj}
------------------------------------------------------------------------------
